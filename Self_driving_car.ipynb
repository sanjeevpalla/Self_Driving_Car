{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Self driving car</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are here building a minimal version of self driving car. Here, we have a front camera view. This will transfer input to the computer. Then Deep Learning algorithm in computer predicts the steering angle to avoid all sorts of collisions. Predicting steering angle can be thought of as a regression problem. We will feed images to Convolutional Neural Network and the label will be the steering angle in that image. Model will learn the steering angle from the as per the turns in the image and will finally predicts steering angle for unknown images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Refer this: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "\n",
    "There are total 45406 images in the dataset along with their steering angles. We will split the dataset into train and test in a ratio of 70:30 sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XHFnthirwlfn"
   },
   "outputs": [],
   "source": [
    "# Credits: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "# Research paper: End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]\n",
    "\n",
    "# NVidia dataset: 72 hrs of video => 72*60*60*30 = 7,776,000 images\n",
    "# Nvidia blog: https://devblogs.nvidia.com/deep-learning-self-driving-cars/\n",
    "\n",
    "\n",
    "# Our Dataset: https://github.com/SullyChen/Autopilot-TensorFlow [https://drive.google.com/file/d/0B-KJCaaF7elleG1RbzVPZWV4Tlk/view]\n",
    "# Size: 25 minutes = 25*60*30 = 45,000 images ~ 2.3 GB\n",
    "\n",
    "\n",
    "# If you want to try on a slightly large dataset: 70 minutes of data ~ 223GB\n",
    "# Refer: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "# Format: Image, latitude, longitude, gear, brake, throttle, steering angles and speed\n",
    "\n",
    "\n",
    "\n",
    "# Additional Installations:\n",
    "# pip3 install h5py\n",
    "\n",
    "\n",
    "# AWS: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "\n",
    "# Youtube:https://www.youtube.com/watch?v=qhUvQiKec2U\n",
    "# Further reading and extensions: https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c\n",
    "# More data: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import scipy.misc\n",
    "import cv2\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivind_data_path = r\"D:/AAIC/Self Driving Car/Autopilot-TensorFlow-master/Autopilot-TensorFlow-master/driving_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "#points to the end of the last batch\n",
    "train_batch_pointer = 0\n",
    "val_batch_pointer = 0\n",
    "\n",
    "#read data.txt\n",
    "with open(driving_data_path+\"data.txt\") as f:\n",
    "    for line in f:\n",
    "        xs.append(driving_data_path + line.split()[0])\n",
    "        #the paper by Nvidia uses the inverse of the turning radius,\n",
    "        #but steering wheel angle is proportional to the inverse of turning radius\n",
    "        #so the steering wheel angle in radians is used as the output\n",
    "        ys.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "#get number of images\n",
    "num_images = len(xs)\n",
    "\n",
    "train_data_size = 0.7\n",
    "train_xs = xs[:int(len(xs) * train_data_size)]\n",
    "train_ys = ys[:int(len(xs) * train_data_size)]\n",
    "\n",
    "val_xs = xs[-int(len(xs) * (1 - train_data_size)):]\n",
    "val_ys = ys[-int(len(xs) * (1 - train_data_size)):]\n",
    "\n",
    "num_train_images = len(train_xs)\n",
    "num_val_images = len(val_xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# PDF of train and test 'y' values. \n",
    "import matplotlib.pyplot as plt \n",
    "plt.hist(train_ys, bins=50, density = True,color='green', histtype ='step');\n",
    "plt.hist(val_ys, bins=50, density = True,color='red', histtype ='step');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(cv2.resize(imageio.imread(train_xs[(train_batch_pointer + i) % num_train_images])[-150:], (200, 66)) / 255.0)\n",
    "        y_out.append([train_ys[(train_batch_pointer + i) % num_train_images]])\n",
    "    train_batch_pointer += batch_size\n",
    "    return x_out, y_out\n",
    "\n",
    "def LoadValBatch(batch_size):\n",
    "    global val_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(cv2.resize(imageio.imread(val_xs[(val_batch_pointer + i) % num_val_images])[-150:], (200, 66)) / 255.0)\n",
    "        y_out.append([val_ys[(val_batch_pointer + i) % num_val_images]])\n",
    "    val_batch_pointer += batch_size\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model utility functions\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-37b76ca9e1b2>:45: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "x = tf.placeholder(tf.float32, shape=[None, 66, 200, 3])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "x_image = x\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "y = tf.identity(tf.matmul(h_fc4_drop, W_fc5) + b_fc5) #scale the linear output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Model_Architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = './save'\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(model.y_, model.y))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver(write_version = saver_pb2.SaverDef.V1)\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "lst_epochs = []\n",
    "lst_steps = []\n",
    "lst_losses = []\n",
    "#epoch_loss = []\n",
    "# train over the dataset about 30 times\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(driving_data.num_images/batch_size)):\n",
    "        xs, ys = driving_data.LoadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict={model.x: xs, model.y_: ys, model.keep_prob: 0.5})\n",
    "        if i % 10 == 0:\n",
    "            xs, ys = driving_data.LoadValBatch(batch_size)\n",
    "            loss_value = loss.eval(feed_dict={model.x:xs, model.y_: ys, model.keep_prob: 1.0})\n",
    "            print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "            lst_epochs.append(epoch)\n",
    "            lst_steps.append(epoch * batch_size + i)\n",
    "            lst_losses.append(loss_value)\n",
    "        # write logs at every iteration\n",
    "        summary = merged_summary_op.eval(feed_dict={model.x:xs, model.y_: ys, model.keep_prob: 1.0})\n",
    "        summary_writer.add_summary(summary, epoch * driving_data.num_images/batch_size + i)\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            if not os.path.exists(LOGDIR):\n",
    "                os.makedirs(LOGDIR)\n",
    "            checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
    "            filename = saver.save(sess, checkpoint_path)\n",
    "        print(\"Model saved in file: %s\" % filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model output\n",
    "dict = {'Epoch':lst_epochs,'Step':lst_steps,'Loss':lst_losses}\n",
    "df = pd.DataFrame(dict)\n",
    "df.to_csv(\"Output.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Step</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.288713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>6.171926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>6.123745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>6.112089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>6.343522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>29</td>\n",
       "      <td>3310</td>\n",
       "      <td>0.182739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>29</td>\n",
       "      <td>3320</td>\n",
       "      <td>0.849366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>29</td>\n",
       "      <td>3330</td>\n",
       "      <td>0.264896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>29</td>\n",
       "      <td>3340</td>\n",
       "      <td>0.563566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>29</td>\n",
       "      <td>3350</td>\n",
       "      <td>0.244751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1380 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Epoch  Step      Loss\n",
       "0         0     0  6.288713\n",
       "1         0    10  6.171926\n",
       "2         0    20  6.123745\n",
       "3         0    30  6.112089\n",
       "4         0    40  6.343522\n",
       "...     ...   ...       ...\n",
       "1375     29  3310  0.182739\n",
       "1376     29  3320  0.849366\n",
       "1377     29  3330  0.264896\n",
       "1378     29  3340  0.563566\n",
       "1379     29  3350  0.244751\n",
       "\n",
       "[1380 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model.ckpt\n",
      "Starting frameofvideo:31785\n",
      "Steering angle: -1.809407397699469 (pred)\t-28.34 (actual)\n",
      "Steering angle: -1.4587672569389936 (pred)\t-28.840000000000003 (actual)\n",
      "Steering angle: -0.6214949208353274 (pred)\t-29.75 (actual)\n",
      "Steering angle: -0.28267058045215115 (pred)\t-31.06 (actual)\n",
      "Steering angle: -0.9685218915248659 (pred)\t-32.27 (actual)\n",
      "Steering angle: 0.011117626978076693 (pred)\t-33.48 (actual)\n",
      "Steering angle: 1.0189487512802469 (pred)\t-34.39 (actual)\n",
      "Steering angle: 2.891273717179758 (pred)\t-35.599999999999994 (actual)\n",
      "Steering angle: 3.0659252827949803 (pred)\t-36.5 (actual)\n",
      "Steering angle: 5.004526562418887 (pred)\t-37.61 (actual)\n",
      "Steering angle: 5.158972080591684 (pred)\t-38.62 (actual)\n",
      "Steering angle: 6.593971346436091 (pred)\t-39.63 (actual)\n",
      "Steering angle: 7.278358008818661 (pred)\t-39.93000000000001 (actual)\n",
      "Steering angle: 6.9975211207152475 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 6.549335206436412 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 4.259369572556597 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 5.459732203617203 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 7.35209416977718 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 7.473649338888605 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 6.5258760679583565 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 13.259457334803155 (pred)\t-40.03000000000001 (actual)\n",
      "Steering angle: 11.278386579250343 (pred)\t-40.24 (actual)\n",
      "Steering angle: 14.063327911431344 (pred)\t-40.93999999999999 (actual)\n",
      "Steering angle: 14.116146618056698 (pred)\t-41.14 (actual)\n",
      "Steering angle: 15.069318299752196 (pred)\t-41.14 (actual)\n",
      "Steering angle: 15.004866927196662 (pred)\t-41.14 (actual)\n",
      "Steering angle: 13.915317712117167 (pred)\t-41.34 (actual)\n",
      "Steering angle: 11.361696957877928 (pred)\t-41.45 (actual)\n",
      "Steering angle: 12.656935612265064 (pred)\t-41.45 (actual)\n",
      "Steering angle: 13.552108742623398 (pred)\t-41.45 (actual)\n",
      "Steering angle: 10.38505418487333 (pred)\t-41.45 (actual)\n",
      "Steering angle: 9.026009610807225 (pred)\t-41.45 (actual)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"save/model.ckpt\")\n",
    "\n",
    "img = cv2.imread('steering_wheel_image.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "smoothed_angle = 0\n",
    "\n",
    "\n",
    "#read data.txt\n",
    "xs = []\n",
    "ys = []\n",
    "with open(drivind_data_path+\"data.txt\") as f:\n",
    "    for line in f:\n",
    "        xs.append(drivind_data_path + line.split()[0])\n",
    "        #the paper by Nvidia uses the inverse of the turning radius,\n",
    "        #but steering wheel angle is proportional to the inverse of turning radius\n",
    "        #so the steering wheel angle in radians is used as the output\n",
    "        ys.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "#get number of images\n",
    "num_images = len(xs)\n",
    "\n",
    "\n",
    "i = math.ceil(num_images*train_data_size)\n",
    "print(\"Starting frameofvideo:\" +str(i))\n",
    "\n",
    "while(cv2.waitKey(10) != ord('q')):\n",
    "    full_image = imageio.imread(drivind_data_path + str(i) + \".jpg\")\n",
    "    image = cv2.resize(full_image[-150:], (200, 66)) / 255.0\n",
    "    degrees = y.eval(feed_dict={x: [image], keep_prob: 1.0})[0][0] * 180.0 / scipy.pi\n",
    "    #call(\"clear\")\n",
    "    #print(\"Predicted Steering angle: \" + str(degrees))\n",
    "    print(\"Steering angle: \" + str(degrees) + \" (pred)\\t\" + str(ys[i]*180/scipy.pi) + \" (actual)\")\n",
    "    cv2.imshow(\"frame\", cv2.cvtColor(full_image, cv2.COLOR_RGB2BGR))\n",
    "    #make smooth angle transitions by turning the steering wheel based on the difference of the current angle\n",
    "    #and the predicted angle\n",
    "    smoothed_angle += 0.2 * pow(abs((degrees - smoothed_angle)), 2.0 / 3.0) * (degrees - smoothed_angle) / abs(degrees - smoothed_angle)\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),-smoothed_angle,1)\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    cv2.imshow(\"steering wheel\", dst)\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Self_driving_car.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
